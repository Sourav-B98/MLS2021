# -*- coding: utf-8 -*-
"""ClassTest

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mh9n14GisC3L9cO9_f0mS9-2R7vN-la5
"""

import torch
import numpy as np
from torch.utils.data import DataLoader
import torchvision
import pathlib
import pandas as pd

train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

!git clone https://github.com/YoongiKim/CIFAR-10-images

from torchvision import datasets
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler

# number of subprocesses to use for data loading
num_workers = 0
# how many samples per batch to load
batch_size = 20
# percentage of training set to use as validation
valid_size = 0.2

# convert data to a normalized torch.FloatTensor
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(), # randomly flip and rotate
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

import os
p=[]
c=[]

for file_name in os.listdir("CIFAR-10-images/test/"):
  for files in os.listdir("CIFAR-10-images/test/"+file_name):
    if files.split(".")[-1].lower() in {"jpeg", "jpg", "png"}:
        path='CIFAR-10-images/test/'+file_name+'/'+files
        clss=file_name
        p.append(path)
        c.append(clss)

p1=[]
c1=[]

for file_name in os.listdir("CIFAR-10-images/train/"):
  for files in os.listdir("CIFAR-10-images/train/"+file_name):
    if files.split(".")[-1].lower() in {"jpeg", "jpg", "png"}:
        path='CIFAR-10-images/train/'+file_name+'/'+files
        clss=file_name
        p1.append(path)
        c1.append(clss)


import pandas as pd
test_dataset=pd.DataFrame({'path':p,'class':c})
train_dataset=pd.DataFrame({'path':p1,'class':c1})

test_d = test_dataset.to_csv('test_data.csv', index=False)
train_d = train_dataset.to_csv('train_data.csv', index=False)

import imageio

class MyDataset():
  def __init__(self,image_set,argument=True):
    with open(image_set,"r") as csv_handle:
      csv_reader = csv.reader(csv_handle,delimiter=",")
      self.imgfiles=[eachline[0] for eachline in csv_reader]
    self.argument=argument
  def __len__(self):
    return len(self.imgfiles)
  def __gititem__(self,idx):
    img=imageio.imread(self.imgfiles[idx])
    X=np.asarray(img,dtype=np.float32)
    if self.argument:
      X=do_yarn_transform(X)
    Y=self.classlabels[idx]
    return X,Y

import csv
train_data=MyDataset('train_data.csv')
test_data=MyDataset('test_data.csv')

# obtain training indices that will be used for validation
num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx, valid_idx = indices[split:], indices[:split]
# define samplers for obtaining training and validation batches
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

# prepare data loaders (combine dataset and sampler)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,
    sampler=train_sampler, num_workers=num_workers)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, 
    sampler=valid_sampler, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
    num_workers=num_workers)
# specify the image classes
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck']

